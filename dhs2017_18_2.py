# -*- coding: utf-8 -*-
"""DHS2017_18-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U38Z9y3dI-VGrgGYq9475PG5JQ8zxr2O
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Get the absolute path of the current folder
abspath_curr = '/content/drive/My Drive/Colab Notebooks/'

"""## Notebook Variables"""

extraColumns = ['land', 'house', 'landarea', 'memsleep']
categoryColumns = ['hv201', 'hv205', 'hv226', 'hv213', 'hv214', 'hv215']
yesNoColumns = ['hv206', 'hv207', 'hv208', 'hv209', 'hv210', 'hv225', 'hv243a', 'hv243b', 'hv211', \
                'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', \
                'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g',
                'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g']

column_name_mapping = {
    'piped into dwelling': 'hv201_11',
    'rainwater': 'hv201_51',
    'protected well': 'hv201_31',
    'piped to neighbor': 'hv201_13',
    'protected spring': 'hv201_41',
    'cart with small tank': 'hv201_71',
    'piped to yard/plot': 'hv201_12',
    'bottled water': 'hv201_91',
    'unprotected spring': 'hv201_42',
    'tube well or borehole': 'hv201_21',
    'filtration plant': 'hv201_96',
    'unprotected well': 'hv201_32',
    'river/dam/lake/ponds/stream/canal/irrigation channel': 'hv201_81',
    'tanker truck': 'hv201_61',
    'public tap/standpipe': 'hv201_14',
    'pit latrine with slab': 'hv205_22',
    'flush to piped sewer system': 'hv205_11',
    'ventilated improved pit latrine (vip)': 'hv205_21',
    'flush to septic tank': 'hv205_12',
    'hanging toilet/latrine': 'hv205_51',
    'flush to pit latrine': 'hv205_13',
    'pit latrine without slab/open pit': 'hv205_23',
    "flush, don't know where": 'hv205_15',
    'no facility/bush/field': 'hv205_61',
    'bucket toilet': 'hv205_41',
    'flush to somewhere else': 'hv205_14',
    'composting toilet': 'hv205_31',
    'straw/shrubs/grass': 'hv226_9',
    'kerosene': 'hv226_5',
    'electricity': 'hv226_1',
    'animal dung': 'hv226_11',
    'wood': 'hv226_8',
    'no food cooked in house': 'hv226_95',
    'natural gas': 'hv226_3',
    'agricultural crop': 'hv226_10',
    'coal, lignite': 'hv226_6',
    'charcoal': 'hv226_7',
    'lpg': 'hv226_2',
    'biogas': 'hv226_4',
    'cement': 'hv213_34',
    'chips/terrazzo': 'hv213_36',
    'mats': 'hv213_38',
    'earth/sand': 'hv213_11',
    'bricks': 'hv213_37',
    'parquet or polished wood': 'hv213_31',
    'wood planks': 'hv213_21',
    'vinyl or asphalt strips': 'hv213_32',
    'palm/bamboo': 'hv213_22',
    'dung': 'hv213_12',
    'carpet': 'hv213_35',
    'marble': 'hv213_39',
    'ceramic tiles': 'hv213_33',
    'stone with mud': 'hv214_23',
    'cement': 'hv214_31',
    'cane/palm/trunks': 'hv214_12',
    'plywood': 'hv214_25',
    'cement blocks': 'hv214_34',
    'bricks': 'hv214_33',
    'mud/stones': 'hv214_14',
    'stone with lime/cement': 'hv214_32',
    'wood planks/shingles': 'hv214_36',
    'covered adobe': 'hv214_35',
    'unbaked bricks/mud': 'hv214_21',
    'uncovered adobe': 'hv214_24',
    'no walls': 'hv214_11',
    'bamboo with mud': 'hv214_22',
    'dirt': 'hv214_13',
    'bamboo/sticks/mud': 'hv214_15',
    'reused wood': 'hv214_26',
    'calamine/cement fiber': 'hv215_35',
    'asbestos': 'hv215_31',
    'wood planks': 'hv215_23',
    'cement/rcc': 'hv215_37',
    'wood': 'hv215_34',
    'rustic mat': 'hv215_21',
    'sod/grass': 'hv215_13',
    'palm/bamboo': 'hv215_22',
    'metal': 'hv215_33',
    'thatch/palm leaf': 'hv215_12',
    'reinforced brick cement/rcc': 'hv215_32',
    'no roof': 'hv215_11',
    'roofing shingles': 'hv215_38',
    'cardboard': 'hv215_24',
    'ceramic tiles': 'hv215_36'
}

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

dhs_data = pd.read_stata(abspath_curr + '/data/PKHR71DT/PKHR71FL.DTA')
#dhs_data = pd.read_stata('./PKHR71DT/PKHR71FL.DTA')

dhs_data1 = pd.read_stata(abspath_curr + '/data/PKBR71DT/PKBR71FL.DTA', convert_categoricals=False)
#dhs_data1 = pd.read_stata('./PKBR71DT/PKBR71FL.DTA', convert_categoricals=False)

# Create variables domestic, house, and land
dhs_data['domestic'] = 0
dhs_data['house'] = 0
dhs_data['land'] = 0

dhs_data = dhs_data[dhs_data['hv015'] != 1]

# Check conditions and update the new columns

df2_condition = (dhs_data1['v716'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v704'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v745a'] >= 1) & (dhs_data1['v745a'] <= 3)
dhs_data.loc[df2_condition, 'house'] = 1

df2_condition = (dhs_data1['v745b'] >= 1) & (dhs_data1['v745b'] <= 3)
dhs_data.loc[df2_condition, 'land'] = 1

df2_condition = (dhs_data1['v740'] == 1)
dhs_data.loc[df2_condition, 'land'] = 1

# Step 1: If (hv012=0), hv012=hv013
dhs_data.loc[dhs_data['hv012'] == 0, 'hv012'] = dhs_data['hv013']

# Step 2: If (hv216>0), memsleep=trunc(hv012/hv216)
dhs_data['memsleep'] = dhs_data.apply(lambda row: int(row['hv012'] / row['hv216']) if row['hv216'] > 0 else row['hv012'], axis=1)

# Step 3: If (hv216=0), memsleep=hv012
dhs_data.loc[dhs_data['hv216'] == 0, 'memsleep'] = dhs_data['hv012']

# Step 4: If (memsleep>=98), memsleep=98
dhs_data.loc[dhs_data['memsleep'] >= 98, 'memsleep'] = 98

# Display the resulting DataFrame or save it to a CSV file
# print(dhs_ata)

# If you want to save the modified DataFrame to a CSV file, you can use:
# df.to_csv('output_data.csv', index=False)

# Create a new DataFrame or use your existing DataFrame (dhs_data)
# Assuming hv244 and hv245 are columns in your DataFrame
dhs_data['landarea'] = 0.0  # Initialize 'landarea' column with 0.0

# Convert 'hv245' to a numeric type (assuming it contains numeric values)
dhs_data['hv245'] = pd.to_numeric(dhs_data['hv245'], errors='coerce')

# Apply conditions to calculate 'landarea'
dhs_data.loc[dhs_data['hv244'] != 1, 'landarea'] = 0
dhs_data.loc[~dhs_data['hv245'].isna(), 'landarea'] = dhs_data['hv245'] / 10
dhs_data.loc[(dhs_data['hv244'] == 1) & (dhs_data['hv245'] == 0), 'landarea'] = 0.5
dhs_data.loc[(dhs_data['hv245'].isna()) | (dhs_data['hv245'] == 998), 'landarea'] = np.nan

"""## Add Columns From Categories"""

def addColumnsFromColumnsWithCategory(df, columns):
    addedColumns = []
    for column in columns:
        categories = set(str(x) for x in df[column] if str(x)!='nan' and str(x)!='other' )
        # print(column, " -> ", categories)
        for category in categories:
            addedColumns.append(category)
            df[category] = df[column].apply(lambda x: 1 if x == category else 0)
    return [df, addedColumns]


def addColumnsFromColumnsWithYesNo(df, columns):
    for column in columns:
        df[column] = df[column].astype(str).str.strip().str.lower()
        df[column] = df[column].apply(lambda x: 1 if x == 'yes' else (1 if x == '1' else 0))
    return df



addColsResults = addColumnsFromColumnsWithCategory(dhs_data , categoryColumns)
dhs_data_req = addColsResults[0]
addedColumns = addColsResults[1]
dhs_data_req = addColumnsFromColumnsWithYesNo(dhs_data_req, yesNoColumns)

requiredColumns = addedColumns + yesNoColumns + extraColumns
print(dhs_data_req)



"""## Summary of Common Variables"""

means = list(dhs_data_req[requiredColumns].mean())
sd = list(dhs_data_req[requiredColumns].std())
newList = []
for i in range(len(requiredColumns)):
  newList.append((requiredColumns[i], means[i], sd[i]))

summaryCommonVarDF = pd.DataFrame(newList, columns=('Variables', 'Mean', 'SD'))
summaryCommonVarDF

summaryCommonVarDF[summaryCommonVarDF['Variables'].isin(yesNoColumns)]

"""## Summary of RURAL"""

dhs_data_rural = dhs_data_req[dhs_data_req['hv025']=='rural'][requiredColumns]

means = list(dhs_data_rural.mean())
sd = list(dhs_data_rural.std())


newList = []

for i in range(len(requiredColumns)):
    newList.append((requiredColumns[i], means[i], sd[i]))
summaryRuralDF = pd.DataFrame(newList, columns=('column', 'mean', 'sd'))
summaryRuralDF



"""## Summary of URBAN"""

dhs_data_urban = dhs_data_req[dhs_data_req['hv025']=='urban'][requiredColumns]

means = list(dhs_data_urban.mean())
sd = list(dhs_data_urban.std())


newList = []

for i in range(len(requiredColumns)):
    newList.append((requiredColumns[i], means[i], sd[i]))
summaryUrbanDF = pd.DataFrame(newList, columns=('column', 'mean', 'sd',))
summaryUrbanDF

summaryUrbanDF['Variables'] = summaryUrbanDF['column'].map(column_name_mapping).fillna(summaryUrbanDF['column'])
summaryUrbanDF

"""## PCA for Common Variables

"""

# Extract the selected columns
selected_variables = dhs_data_req[summaryCommonVarDF['Variables']].copy()

# Drop rows with missing values
selected_variables = selected_variables.dropna()

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_CommonVariables_basis_vector.csv', index=False)

"""## PCA for URBAN

"""

# Extract the selected columns
selected_variables = dhs_data_req[summaryCommonVarDF['Variables']].copy()
selected_variables = dhs_data_req[dhs_data_req['hv025']=='urban'][requiredColumns]

# Drop rows with missing values
selected_variables = selected_variables.dropna()

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_Urban_basis_vector.csv', index=False)

"""## PCA for RURAL


"""

# Extract the selected columns
selected_variables = dhs_data_req[summaryCommonVarDF['Variables']].copy()
selected_variables = dhs_data_req[dhs_data_req['hv025']=='rural'][requiredColumns]

# Drop rows with missing values
selected_variables = selected_variables.dropna()

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_Rural_basis_vector.csv', index=False)

#from google.colab import files
#basis_vector.to_csv('asset_index_Rural_basis_vector.csv', index=False)
#files.download('asset_index_Rural_basis_vector.csv')

def factorAnalysis(df, score):
    pass

def linearRegression(df, score):
    pass

def calculateCombinedWealthScore(df, score):
    pass

def tabulationForHistogram(df, score):
    pass





