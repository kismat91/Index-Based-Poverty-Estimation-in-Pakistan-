# -*- coding: utf-8 -*-
"""DHS2017_18-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U38Z9y3dI-VGrgGYq9475PG5JQ8zxr2O
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Get the absolute path of the current folder
abspath_curr = '/content/drive/My Drive/Colab Notebooks/'

"""## Indicator Variables"""

extraColumns = ['land', 'house', 'landarea', 'memsleep']
categoryColumns = ['hv201', 'hv205', 'hv226', 'hv213', 'hv214', 'hv215']
yesNoColumns = ['hv206', 'hv207', 'hv208', 'hv209', 'hv210', 'hv225', 'hv243a', 'hv243b', 'hv211', \
                'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', \
                'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g',
                'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g']

column_name_mapping = {
    'piped into dwelling': 'hv201_11',
    'rainwater': 'hv201_51',
    'protected well': 'hv201_31',
    'piped to neighbor': 'hv201_13',
    'protected spring': 'hv201_41',
    'cart with small tank': 'hv201_71',
    'piped to yard/plot': 'hv201_12',
    'bottled water': 'hv201_91',
    'unprotected spring': 'hv201_42',
    'tube well or borehole': 'hv201_21',
    'filtration plant': 'hv201_96',
    'unprotected well': 'hv201_32',
    'river/dam/lake/ponds/stream/canal/irrigation channel': 'hv201_81',
    'tanker truck': 'hv201_61',
    'public tap/standpipe': 'hv201_14',
    'pit latrine with slab': 'hv205_22',
    'flush to piped sewer system': 'hv205_11',
    'ventilated improved pit latrine (vip)': 'hv205_21',
    'flush to septic tank': 'hv205_12',
    'hanging toilet/latrine': 'hv205_51',
    'flush to pit latrine': 'hv205_13',
    'pit latrine without slab/open pit': 'hv205_23',
    "flush, don't know where": 'hv205_15',
    'no facility/bush/field': 'hv205_61',
    'bucket toilet': 'hv205_41',
    'flush to somewhere else': 'hv205_14',
    'composting toilet': 'hv205_31',
    'straw/shrubs/grass': 'hv226_9',
    'kerosene': 'hv226_5',
    'electricity': 'hv226_1',
    'animal dung': 'hv226_11',
    'wood': 'hv226_8',
    'no food cooked in house': 'hv226_95',
    'natural gas': 'hv226_3',
    'agricultural crop': 'hv226_10',
    'coal, lignite': 'hv226_6',
    'charcoal': 'hv226_7',
    'lpg': 'hv226_2',
    'biogas': 'hv226_4',
    'cement': 'hv213_34',
    'chips/terrazzo': 'hv213_36',
    'mats': 'hv213_38',
    'earth/sand': 'hv213_11',
    'bricks': 'hv213_37',
    'parquet or polished wood': 'hv213_31',
    'wood planks': 'hv213_21',
    'vinyl or asphalt strips': 'hv213_32',
    'palm/bamboo': 'hv213_22',
    'dung': 'hv213_12',
    'carpet': 'hv213_35',
    'marble': 'hv213_39',
    'ceramic tiles': 'hv213_33',
    'stone with mud': 'hv214_23',
    'cement': 'hv214_31',
    'cane/palm/trunks': 'hv214_12',
    'plywood': 'hv214_25',
    'cement blocks': 'hv214_34',
    'bricks': 'hv214_33',
    'mud/stones': 'hv214_14',
    'stone with lime/cement': 'hv214_32',
    'wood planks/shingles': 'hv214_36',
    'covered adobe': 'hv214_35',
    'unbaked bricks/mud': 'hv214_21',
    'uncovered adobe': 'hv214_24',
    'no walls': 'hv214_11',
    'bamboo with mud': 'hv214_22',
    'dirt': 'hv214_13',
    'bamboo/sticks/mud': 'hv214_15',
    'reused wood': 'hv214_26',
    'calamine/cement fiber': 'hv215_35',
    'asbestos': 'hv215_31',
    'wood planks': 'hv215_23',
    'cement/rcc': 'hv215_37',
    'wood': 'hv215_34',
    'rustic mat': 'hv215_21',
    'sod/grass': 'hv215_13',
    'palm/bamboo': 'hv215_22',
    'metal': 'hv215_33',
    'thatch/palm leaf': 'hv215_12',
    'reinforced brick cement/rcc': 'hv215_32',
    'no roof': 'hv215_11',
    'roofing shingles': 'hv215_38',
    'cardboard': 'hv215_24',
    'ceramic tiles': 'hv215_36'
}

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler

dhs_data = pd.read_stata(abspath_curr + '/data/PKHR71DT/PKHR71FL.DTA')
#dhs_data = pd.read_stata('./PKHR71DT/PKHR71FL.DTA')

from pandas.core.tools.datetimes import unique
test= dhs_data['shdist']
test1= unique(test)
print(test1)
len(test1)

!pip install pyreadstat
import pyreadstat

# Assuming you have a Stata dataset file named 'your_data.dta'
df, meta = pyreadstat.read_dta(abspath_curr + '/data/PKHR71DT/PKHR71FL.DTA')

# Access variable labels from the metadata
variable_labels = meta.column_labels

# Print variable labels
for i, var_label in enumerate(variable_labels):
    var_name = df.columns[i]
    print(f"Variable: {var_name}, Label: {var_label}")

from pandas.core.tools.datetimes import unique
test= dhs_data['hv024']
test1= unique(test)
print(test1)
len(test1)

dhs_data1 = pd.read_stata(abspath_curr + '/data/PKBR71DT/PKBR71FL.DTA', convert_categoricals=False)
#dhs_data1 = pd.read_stata('./PKBR71DT/PKBR71FL.DTA', convert_categoricals=False)

# Create variables domestic, house, and land
dhs_data['domestic'] = 0
dhs_data['house'] = 0
dhs_data['land'] = 0

dhs_data = dhs_data[dhs_data['hv015'] != 1]

# Check conditions and update the new columns

df2_condition = (dhs_data1['v716'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v704'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v745a'] >= 1) & (dhs_data1['v745a'] <= 3)
dhs_data.loc[df2_condition, 'house'] = 1

df2_condition = (dhs_data1['v745b'] >= 1) & (dhs_data1['v745b'] <= 3)
dhs_data.loc[df2_condition, 'land'] = 1

df2_condition = (dhs_data1['v740'] == 1)
dhs_data.loc[df2_condition, 'land'] = 1

# Step 1: If (hv012=0), hv012=hv013
dhs_data.loc[dhs_data['hv012'] == 0, 'hv012'] = dhs_data['hv013']

# Step 2: If (hv216>0), memsleep=trunc(hv012/hv216)
dhs_data['memsleep'] = dhs_data.apply(lambda row: int(row['hv012'] / row['hv216']) if row['hv216'] > 0 else row['hv012'], axis=1)

# Step 3: If (hv216=0), memsleep=hv012
dhs_data.loc[dhs_data['hv216'] == 0, 'memsleep'] = dhs_data['hv012']

# Step 4: If (memsleep>=98), memsleep=98
dhs_data.loc[dhs_data['memsleep'] >= 98, 'memsleep'] = 98

# Display the resulting DataFrame or save it to a CSV file
# print(dhs_ata)

# If you want to save the modified DataFrame to a CSV file, you can use:
# df.to_csv('output_data.csv', index=False)

# Create a new DataFrame or use your existing DataFrame (dhs_data)
# Assuming hv244 and hv245 are columns in your DataFrame
dhs_data['landarea'] = 0.0  # Initialize 'landarea' column with 0.0

# Convert 'hv245' to a numeric type (assuming it contains numeric values)
dhs_data['hv245'] = pd.to_numeric(dhs_data['hv245'], errors='coerce')

# Apply conditions to calculate 'landarea'
dhs_data.loc[dhs_data['hv244'] != 1, 'landarea'] = 0
dhs_data.loc[~dhs_data['hv245'].isna(), 'landarea'] = dhs_data['hv245'] / 10
dhs_data.loc[(dhs_data['hv244'] == 1) & (dhs_data['hv245'] == 0), 'landarea'] = 0.5
dhs_data.loc[(dhs_data['hv245'].isna()) | (dhs_data['hv245'] == 998), 'landarea'] = np.nan

"""## Add Columns From Categories"""

def addColumnsFromColumnsWithCategory(df, columns):
    addedColumns = []
    for column in columns:
        categories = set(str(x) for x in df[column] if str(x)!='nan' and str(x)!='other' )
        # print(column, " -> ", categories)
        for category in categories:
            addedColumns.append(category)
            df[category] = df[column].apply(lambda x: 1 if x == category else 0)
    return [df, addedColumns]


def addColumnsFromColumnsWithYesNo(df, columns):
    for column in columns:
        df[column] = df[column].astype(str).str.strip().str.lower()
        df[column] = df[column].apply(lambda x: 1 if x == 'yes' else (1 if x == '1' else 0))
    return df



addColsResults = addColumnsFromColumnsWithCategory(dhs_data , categoryColumns)
dhs_data_req = addColsResults[0]
addedColumns = addColsResults[1]
dhs_data_req = addColumnsFromColumnsWithYesNo(dhs_data_req, yesNoColumns)

requiredColumns = addedColumns + yesNoColumns + extraColumns
print(dhs_data_req)



"""## Summary of Common Variables"""

means = list(dhs_data_req[requiredColumns].mean())
sd = list(dhs_data_req[requiredColumns].std())
newList = []
for i in range(len(requiredColumns)):
  newList.append((requiredColumns[i], means[i], sd[i]))

summaryCommonVarDF = pd.DataFrame(newList, columns=('Variables', 'Mean', 'SD'))
summaryCommonVarDF

summaryCommonVarDF[summaryCommonVarDF['Variables'].isin(yesNoColumns)]

"""## Summary of RURAL"""

dhs_data_rural = dhs_data_req[dhs_data_req['hv025']=='rural'][requiredColumns]

means = list(dhs_data_rural.mean())
sd = list(dhs_data_rural.std())


newList = []

for i in range(len(requiredColumns)):
    newList.append((requiredColumns[i], means[i], sd[i]))
summaryRuralDF = pd.DataFrame(newList, columns=('column', 'mean', 'sd'))
summaryRuralDF



"""## Summary of URBAN"""

dhs_data_urban = dhs_data_req[dhs_data_req['hv025']=='urban'][requiredColumns]

means = list(dhs_data_urban.mean())
sd = list(dhs_data_urban.std())


newList = []

for i in range(len(requiredColumns)):
    newList.append((requiredColumns[i], means[i], sd[i]))
summaryUrbanDF = pd.DataFrame(newList, columns=('column', 'mean', 'sd',))
summaryUrbanDF

summaryUrbanDF['Variables'] = summaryUrbanDF['column'].map(column_name_mapping).fillna(summaryUrbanDF['column'])
summaryUrbanDF

"""## PCA for Common Variables

"""

# Extract the selected columns
selected_variables = dhs_data_req[requiredColumns].copy()

# Drop rows with missing values
selected_variables = selected_variables.fillna(0)

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_CommonVariables_basis_vector.csv', index=False)

# Add the wealth index as a new column in your DataFrame
dhs_data_req['Wealth_Index1'] = asset_index

# Save the updated DataFrame with the wealth index to a new CSV file
dhs_data_req.to_csv('DHS_data_with_wealth_index.csv', index=False)

dhs_data_req['Wealth_Index1']

# Load the DHS dataset
dhs_data = pd.read_csv('DHS_data_with_wealth_index.csv')

# Rename the province and urban/rural columns for clarity
dhs_data = dhs_data.rename(columns={'hv024': 'Province', 'hv025': 'Urban_Rural'})

# Map the numerical codes to actual province names and urban/rural categories
province_mapping = {1: 'kpk', 2: 'punjab', 3: 'sindh', 4: 'balochistan', 5: 'fata', 6: 'ict', 7: 'gb', 8: 'ajk'}
urban_rural_mapping = {1: 'urban', 2: 'rural'}

dhs_data['Province'] = dhs_data['Province'].replace(province_mapping)
dhs_data['Urban_Rural'] = dhs_data['Urban_Rural'].replace(urban_rural_mapping)

# Calculate distribution statistics for the wealth index in the DHS dataset
dhs_wealth_index_stats_national = dhs_data['Wealth_Index1'].describe()
dhs_wealth_index_stats_province = dhs_data.groupby('Province')['Wealth_Index1'].describe()
dhs_wealth_index_stats_urban_rural = dhs_data.groupby('Urban_Rural')['Wealth_Index1'].describe()
dhs_wealth_index_stats_province_urban_rural = dhs_data.groupby(['Province', 'Urban_Rural'])['Wealth_Index1'].describe()

# Display the results
print("National Level:")
print(dhs_wealth_index_stats_national)
print("\nBy Province:")
print(dhs_wealth_index_stats_province)
print("\nBy Urban/Rural:")
print(dhs_wealth_index_stats_urban_rural)
print("\nBy Province and Urban/Rural:")
print(dhs_wealth_index_stats_province_urban_rural)

from google.colab import files
# dhs_data_req.to_csv('DHS_data_with_wealth_index.csv', index=False)
# files.download('DHS_data_with_wealth_index.csv')

dhs_data_req['shdist']

import geopandas as gpd
gdf = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm2_wfp_20220909.shp')

# Convert the district names to lowercase
gdf['ADM2_EN'] = gdf['ADM2_EN'].str.lower()
dhs_data_req['shdist'] = dhs_data_req['shdist'].str.lower()

# Left join on the district names
merged_data = gdf.merge(dhs_data_req, left_on='ADM2_EN', right_on='shdist', how='left')

# Fill NaN values with a default value (e.g., 0) or use another suitable method like interpolation
merged_data['Wealth_Index1'].fillna(0, inplace=True)

# Plotting the heatmap
fig, ax = plt.subplots(figsize=(12, 10))
merged_data.plot(column='Wealth_Index1', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
plt.title("Wealth index of districts in Pakistan")
plt.tight_layout()
plt.show()

Provinces  = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm1_wfp_20220909.shp')

# Create a mapping between province names
province_mapping = {
    'Azad Kashmir': 'ajk',
    'Balochistan': 'balochistan',
    'Gilgit Baltistan': 'gb',
    'Islamabad': 'ict',
    'Khyber Pakhtunkhwa': 'kpk',
    'Punjab': 'punjab',
    'Sindh': 'sindh'
}

# Create a new column 'Province_Short' in the Provinces dataframe with the short form of the province name
Provinces['Province_Short'] = Provinces['ADM1_EN'].map(province_mapping)

# Left join on the province short names
merged_data = Provinces.merge(dhs_data_req, left_on='Province_Short', right_on='hv024', how='left')

# Fill NaN values with a default value (e.g., 0)
merged_data['Wealth_Index1'].fillna(0, inplace=True)

# Plotting the heatmap
fig, ax = plt.subplots(figsize=(12, 10))
merged_data.plot(column='Wealth_Index1', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
plt.title("Wealth index of provinces in Pakistan")
plt.tight_layout()
plt.show()

National  = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm0_wfp_20220909.shp')

# Create a mapping between province names
national_mapping = {
   'PK': 'PK7'
}

# Create a new column 'Province_Short' in the Provinces dataframe with the short form of the province name
National['national_Short'] = National['ADM0_PCODE'].map(national_mapping)

# Left join on the province short names
merged_data = National.merge(dhs_data_req, left_on='national_Short', right_on='hv000', how='left')

# Fill NaN values with a default value (e.g., 0)
#merged_data['Wealth_Index1'].fillna(0, inplace=True)

# Plotting the heatmap
fig, ax = plt.subplots(figsize=(12, 10))
merged_data.plot(column='Wealth_Index1', cmap='coolwarm', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
plt.title("Wealth index of provinces in Pakistan")
plt.tight_layout()
plt.show()



"""## PCA for URBAN

"""

# Extract the selected columns
selected_variables = dhs_data_req[summaryCommonVarDF['Variables']].copy()
selected_variables = dhs_data_req[dhs_data_req['hv025']=='urban'][requiredColumns]

# Drop rows with missing values
selected_variables = selected_variables.dropna()

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_Urban_basis_vector.csv', index=False)

"""## PCA for RURAL


"""

# Extract the selected columns
selected_variables = dhs_data_req[summaryCommonVarDF['Variables']].copy()
selected_variables = dhs_data_req[dhs_data_req['hv025']=='rural'][requiredColumns]

# Drop rows with missing values
selected_variables = selected_variables.dropna()

# Perform Min-Max scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(selected_variables)

# Perform PCA
pca = PCA(n_components=1)
asset_index = pca.fit_transform(scaled_data)

# Print the explained variance
print('PCA variance explained: %.2f%%' % (100 * pca.explained_variance_ratio_[0]))

# Create the basis vector DataFrame
basis_vector = pd.DataFrame({'Asset': summaryCommonVarDF['Variables'], 'Magnitude': pca.components_[0]})

# Sort the basis vector by magnitude in descending order
basis_vector = basis_vector.sort_values(by='Magnitude', ascending=False)

# Save the basis vector to a CSV file
basis_vector.to_csv('asset_index_Rural_basis_vector.csv', index=False)

#from google.colab import files
#basis_vector.to_csv('asset_index_Rural_basis_vector.csv', index=False)
#files.download('asset_index_Rural_basis_vector.csv')





