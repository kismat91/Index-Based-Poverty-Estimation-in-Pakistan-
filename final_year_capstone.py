# -*- coding: utf-8 -*-
"""Final_year_Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-QidExro591u_mQD-XcjDqYPn25ACsgE
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Get the absolute path of the current folder
abspath_curr = '/content/drive/My Drive/Colab Notebooks/'

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder

import warnings

# Ignore warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

# Set matplotlib sizes
plt.rc('font', size=20)
plt.rc('axes', titlesize=20)
plt.rc('axes', labelsize=20)
plt.rc('xtick', labelsize=20)
plt.rc('ytick', labelsize=20)
plt.rc('legend', fontsize=20)
plt.rc('figure', titlesize=20)

import pandas as pd

# Load the RWI data from the CSV file
rwi_data = pd.read_csv(abspath_curr + '/data/pak_rwi.csv')

# Explore the data
print("Data Overview:")
print(rwi_data.head())  # Display the first few rows of the data
print("\nData Statistics:")
print(rwi_data.describe())  # Display basic statistics of the data
print("\nData Information:")
print(rwi_data.info())  # Display data type information and non-null counts

import geopandas as gpd
from shapely.geometry import Point

# Create a GeoDataFrame from the Latitude and Longitude columns
geometry = [Point(lon, lat) for lon, lat in zip(rwi_data['longitude'], rwi_data['latitude'])]
geo_df = gpd.GeoDataFrame(rwi_data, geometry=geometry)

# Plot the spatial distribution of RWI scores
fig, ax = plt.subplots(figsize=(10, 10))
geo_df.plot(column='rwi', cmap='viridis', legend=True, ax=ax)
ax.set_title('Spatial Distribution of RWI Scores in Pakistan and India')
plt.show()

# Load the administrative level shapefile
districts = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm3_wfp_20220909.shp')
print(districts.head())

# Perform a spatial join to aggregate RWI scores by district
merged_data = gpd.sjoin(districts, geo_df, how='left', op='intersects')

# Calculate mean RWI score for each tehsil of each district
district_agg = merged_data.groupby('ADM3_EN')['rwi'].mean().reset_index()
print(district_agg)

# Merge the aggregated data with the administrative boundaries
merged_data = districts.merge(district_agg, left_on='ADM3_EN', right_on='ADM3_EN', how='left')

# Plot the aggregated RWI data
merged_data.plot(column='rwi', cmap='coolwarm', legend=True)
plt.title('Aggregated RWI Scores by tehsil of each District')
plt.show()

districts = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm0_wfp_20220909.shp')
print(districts.head())
district_agg = merged_data.groupby('ADM0_EN')['rwi'].mean().reset_index()
print(district_agg)

# Merge the aggregated data with the administrative boundaries
merged_data = districts.merge(district_agg, left_on='ADM0_EN', right_on='ADM0_EN', how='left')

# Plot the aggregated RWI data
merged_data.plot(column='rwi', cmap='coolwarm', legend=True)
plt.title('Aggregated RWI Scores for Pakistan ')
plt.show()

districts = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm1_wfp_20220909.shp')
print(districts.head())

# Perform a spatial join to aggregate RWI scores by district
merged_data = gpd.sjoin(districts, geo_df, how='left', op='intersects')

district_agg = merged_data.groupby('ADM1_EN')['rwi'].mean().reset_index()
print(district_agg)

# Merge the aggregated data with the administrative boundaries
merged_data = districts.merge(district_agg, left_on='ADM1_EN', right_on='ADM1_EN', how='left')

# Plot the aggregated RWI data
merged_data.plot(column='rwi', cmap='coolwarm', legend=True)
plt.title('Aggregated RWI Scores by Provinces')
plt.show()

districts = gpd.read_file(abspath_curr + '/data/pak_admbnda_adm2_wfp_20220909.shp')
print(districts.head())

# Perform a spatial join to aggregate RWI scores by district
merged_data = gpd.sjoin(districts, geo_df, how='left', op='intersects')

district_agg = merged_data.groupby('ADM2_EN')['rwi'].mean().reset_index()
print(district_agg)
# Merge the aggregated data with the administrative boundaries
merged_data = districts.merge(district_agg, left_on='ADM2_EN', right_on='ADM2_EN', how='left')

# Plot the aggregated RWI data
merged_data.plot(column='rwi', cmap='coolwarm', legend=True)
plt.title('Aggregated RWI Scores by Cities')
plt.show()

import fiona
fiona.os.environ['SHAPE_RESTORE_SHX'] = 'YES'

import pandas as pd
import geopandas as gpd
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load DHS dataset
dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKBQ7ADT/PKBQ7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKCH7ADT/PKCH7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKFW7ADT/PKFW7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKHH7ADT/PKHH7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKIQ7ADT/PKIQ7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKOD7ADT/PKOD7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKPQ7ADT/PKPQ7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKSQ7ADT/PKSQ7AFL.DTA')
dhs_data.head()

dhs_data = pd.read_stata(abspath_curr + '/data/PK_2019_DHS/PKVA7ADT/PKVA7AFL.DTA')
dhs_data.head()

Hies_data = pd.read_stata(abspath_curr + '/data/PSLM_HIES/310_HIES201819_Rescaledbyhhsize_24618obs.dta')
print(Hies_data.head())

import seaborn as sns
corr = dhs_data.corr(method = 'pearson')
plt.figure(figsize=(10,8),dpi=500)
sns.heatmap(corr, annot=True, fmt=".2f", linewidth=.5)
plt.show()

from pandas.compat import numpy


import pandas as pd
import numpy as np


correlation_matrix = dhs_data.corr().abs()

# Get the upper triangle of the correlation matrix (excluding the diagonal)
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Find the top 15 pairs with the highest absolute correlation
top_pairs = upper_triangle.unstack().sort_values(ascending=False).head(15)

# Extract the variable pairs with the highest correlations
best_variable_pairs = top_pairs.index.tolist()

# 'best_variable_pairs' will contain pairs of strongly correlated variables
print(best_variable_pairs)

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Assuming your dataset is stored in a DataFrame
all_variables = dhs_data.columns

# Create an empty DataFrame to store variable pairs and their associations
associations = pd.DataFrame(columns=['Variable1', 'Variable2', 'Association'])

# Loop through all pairs of variables
for i in range(len(all_variables)):
    for j in range(i + 1, len(all_variables)):
        variable1 = all_variables[i]
        variable2 = all_variables[j]

        try:
            # Calculate the association between two variables using Cram√©r's V
            contingency_table = pd.crosstab(dhs_data[variable1], dhs_data[variable2])
            chi2, _, _, _ = chi2_contingency(contingency_table)
            n = contingency_table.sum().sum()
            association = np.sqrt(chi2 / (n * min(contingency_table.shape) - 1))

            # Append the pair and association to the DataFrame
            associations = associations.append({'Variable1': variable1, 'Variable2': variable2, 'Association': association}, ignore_index=True)
        except Exception as e:
            # Handle cases where the contingency table cannot be computed (e.g., no common values)
            pass

# Sort the DataFrame by association in descending order and get the top 15 pairs
top_associations = associations.sort_values(by='Association', ascending=False).head(15)

# 'top_associations' will contain the top 15 variable pairs with the highest associations
print(top_associations)

from google.colab import files
#dhs_data.to_csv('310_HIES201819_Rescaledbyhhsize_24618obs.csv', index=False)
#files.download('310_HIES201819_Rescaledbyhhsize_24618obs.csv')

dhs_data = pd.read_stata(abspath_curr + '/data/PSLM_HIES/310_PSLM_HIES_Rescaledbyhhsize.dta')
dhs_data
#dhs_data.to_csv('310_PSLM_HIES_Rescaledbyhhsize.csv', index=False)
#files.download('310_PSLM_HIES_Rescaledbyhhsize.csv')

import pyreadstat

# Assuming you have a Stata dataset file named 'your_data.dta'
df, meta = pyreadstat.read_dta(abspath_curr + '/data/PSLM_HIES/310_PSLM_HIES_Rescaledbyhhsize.dta')

# Access variable labels from the metadata
variable_labels = meta.column_labels

# Print variable labels
for i, var_label in enumerate(variable_labels):
    var_name = df.columns[i]
    print(f"Variable: {var_name}, Label: {var_label}")

!pip install pyreadstat

Pslm_data = pd.read_stata(abspath_curr + '/data/PSLM_HIES/310_PSLM201920_Rescaledbyhhsize_160654obs.dta')
Pslm_data.head()
data_types = Pslm_data.dtypes
for var_name, data_type in data_types.iteritems():
    print(f"Variable: {var_name}, Data Type: {data_type}")
#dhs_data.to_csv('310_PSLM201920_Rescaledbyhhsize_160654obs.csv', index=False)
#files.download('310_PSLM201920_Rescaledbyhhsize_160654obs.csv')

# Select the 291 variables you want to use for the wealth index
selected_variables = [
    'H_occup_ownwernsh', 'H_occup_ownwersh', 'H_occup_onrent', 'H_occup_rentsubs', 'H_occup_rentfree',
    'H_owner_male', 'H_owner_female', 'H_owner_joinltly', 'H_owner_dk', 'H_house_independ',
    'H_house_aptflat', 'H_house_poflunit', 'H_house_pocompo', 'H_house_other', 'H_numrooms',
    'H_floor_earth', 'H_floor_ceramic', 'H_floor_cement', 'H_floor_bricks', 'H_floor_other',
    'H_roof_rccrbc', 'H_roof_wood', 'H_roof_sheet', 'H_roof_grader', 'H_roof_other',
    'H_walls_burntbricks', 'H_walls_mudbricks', 'H_walls_wood', 'H_walls_stones', 'H_walls_other',
    'H_cooking_firewood', 'H_cooking_gas', 'H_cooking_lpg', 'H_cooking_dung', 'H_cooking_crop',
    'H_cooking_other', 'H_heating_elect', 'H_heating_lpg', 'H_heating_gas', 'H_heating_crop',
    'H_heating_coal', 'H_heating_dung', 'H_heating_nofac', 'H_heating_other', 'H_lighting_elect',
    'H_lighting_solar', 'H_lighting_kerosene', 'H_lighting_candles', 'H_lighting_other', 'H_lnnumrooms',
    'W_dkw_inspiped', 'W_dkw_inshandpump', 'W_dkw_insmotorpump', 'W_dkw_insclosedwell', 'W_dkw_insopenwell',
    'W_dkw_insprotsprng', 'W_dkw_insunprsprng', 'W_dkw_outpiped', 'W_dkw_outhandpump', 'W_dkw_outmotorpump',
    'W_dkw_outclosedwell', 'W_dkw_outopenwell', 'W_dkw_outprotsprng', 'W_dkw_outunprsprng', 'W_dkw_pond',
    'W_dkw_bottwater', 'W_dkw_tanker', 'W_dkw_filtration', 'W_dkw_other', 'W_dkw_piped', 'W_dkw_handpump',
    'W_dkw_motorpump', 'W_dkw_closedwell', 'W_dkw_openwell', 'W_drinkingpay', 'W_dkw_protsprng', 'W_dkw_unprsprng',
    'W_dkw_nodelivery', 'W_dkw_safe', 'W_whoinsgovt', 'W_whoinscomm', 'W_whoinshhits', 'W_whoinspriva',
    'W_whoinsdk', 'W_whoinsnosys', 'W_inside', 'W_distance', 'W_time', 'W_saferyes', 'W_saferno', 'W_saferdk',
    'W_safeboil', 'W_safebleach', 'W_safestrain', 'W_safefilter', 'W_safesettle', 'W_safedk', 'W_safeno',
    'W_drinkwabailableyes', 'W_drinkwabailableno', 'W_drinkwabailabledk', 'W_drinkinghowmuch',
    'W_winllingtopayimp', 'W_notsafenotwilling', 'W_notsafebutwilling', 'W_toilet_notoilet', 'W_toilet_flushpub',
    'W_toilet_flushtank', 'W_toilet_flushpit', 'W_toilet_flushopen', 'W_toilet_raiselat', 'W_toilet_pitlat',
    'W_toilet_other', 'W_opendefecation', 'W_toiletshared', 'W_toiletprivate', 'W_drainage_covered',
    'W_drainage_undergr', 'W_drainage_open', 'W_drainage_nosystem', 'W_ckg_piped', 'W_ckg_handpump',
    'W_ckg_motorpump', 'W_ckg_closedwell', 'W_ckg_openwell', 'W_ckg_protsprng', 'W_ckg_unprsprng', 'W_ckg_pond',
    'W_ckg_bottwater', 'W_ckg_tanker', 'W_ckg_filtration', 'W_ckg_other', 'W_handw_piped', 'W_handw_handpump',
    'W_handw_motorpump', 'W_handw_closedwell', 'W_handw_openwell', 'W_handw_protsprng', 'W_handw_unprsprng',
    'W_handw_pond', 'W_handw_tanker', 'W_handw_other', 'W_handwashingplace', 'W_lntime', 'W_lndkhow',
    'D_nradio', 'D_ntelevsion', 'D_nlcdled', 'D_nrefrigerator', 'D_nfreezer', 'D_nwashing', 'D_ndryer',
    'D_nairconditioning', 'D_naircooler', 'D_nfan', 'D_nstove', 'D_ncookingrange', 'D_nmicrowave',
    'D_nsewingmachine', 'D_nknitting', 'D_niron', 'D_nwaterfilter', 'D_ndonkeypump', 'D_nturbine', 'D_nchair',
    'D_ntable', 'D_nups', 'D_ngenerator', 'D_nsolarpanel', 'D_nheater', 'D_ngeaser', 'D_nbicycle',
    'D_nmotorcyclescotter', 'D_nrichshaw', 'D_ncar', 'D_nvantruckbus', 'D_nboat', 'D_ntractortralloy',
    'D_nclock', 'D_iradio', 'D_itelevsion', 'D_ilcdled', 'D_irefrigerator', 'D_ifreezer', 'D_iwashing',
    'D_idryer', 'D_iairconditioning', 'D_iaircooler', 'D_ifan', 'D_istove', 'D_icookingrange', 'D_imicrowave',
    'D_isewingmachine', 'D_iknitting', 'D_iiron', 'D_iwaterfilter', 'D_idonkeypump', 'D_iturbine', 'D_ichair',
    'D_itable', 'D_iups', 'D_igenerator', 'D_isolarpanel', 'D_iheater', 'D_igeaser', 'D_ibicycle',
    'D_imotorcyclescotter', 'D_irichshaw', 'D_icar', 'D_ivantruckbus', 'D_iboat', 'D_itractortralloy','D_iclock',
    'D_nodurables','C_noeducationinfo', 'C_noemploymentinfo', 'C_female', 'C_single', 'C_married', 'C_widow', 'C_age',
    'C_age2', 'C_lnage', 'C_minor', 'C_senior', 'C_canread', 'C_canreadwrite',
    'C_canreadbnotwrite', 'C_canmath', 'C_neverattended', 'C_attendedpast', 'C_currentattending', 'C_schoolgov',
    'C_schoolpriv', 'C_reasongoodteaching', 'C_reasoncheaper', 'C_reasonnearhome', 'C_noeducation', 'C_basiceducation',
    'C_intermededucation', 'C_higheducation', 'C_highesteducation', 'C_numyearsedu', 'C_employed', 'C_seeking', 'C_occmangprof',
    'C_occtecserv', 'C_occskilled', 'C_occplntcrft', 'C_occelemt', 'C_actagri', 'C_actmanu', 'C_actserv', 'C_paidempl',
    'C_contfamworker', 'C_selfempnonagri', 'C_owncultivator', 'C_secondoccup', 'C_otherwork', 'C_pension', 'F_nmembers',
    'F_nminors', 'F_nadults', 'F_nseniors', 'F_nmarriedmales', 'F_nonmember', 'F_memratio', 'F_depratio', 'F_lnnmembers',
    'F_noeducationinfo', 'F_adultcannotread', 'F_adultcanread', 'F_nnoeducation', 'F_nbasiceducation', 'F_nintermededucation',
    'F_nhigheducation', 'F_nhighesteducation', 'F_nbasicorless', 'F_nbasicormore', 'F_ninterorless', 'F_ninterormore',
    'F_nhighlorless', 'F_nhighlormore', 'F_nnolaborinfo', 'F_nemployed', 'F_nseeking', 'F_nseniornonpensioned', 'F_pensioned',
    'F_2ormoreemployed', 'F_0employed']


# Extract the selected variables from the dataset
selected_data = Pslm_data[selected_variables]

# Filter out non-numeric columns and the categorical column
numeric_columns = selected_data.select_dtypes(include=[int, float]).columns
selected_data_numeric = selected_data[numeric_columns]

# Handle the categorical variable (W_drinkingpay) with one-hot encoding
encoder = OneHotEncoder(sparse=False, drop='first')
encoder = OneHotEncoder(sparse_output=False, drop='first')
drinkingpay_encoded = encoder.fit_transform(Pslm_data[["W_drinkingpay"]])

# Manually create feature names for the one-hot encoded column
drinkingpay_feature_names = [f"W_drinkingpay_{category}" for category in encoder.categories_[0][1:]]

drinkingpay_encoded_df = pd.DataFrame(drinkingpay_encoded, columns=drinkingpay_feature_names)

# Combine the numeric data and the one-hot encoded categorical data
selected_data_combined = pd.concat([selected_data_numeric, drinkingpay_encoded_df], axis=1)

# Remove any rows with missing values (NaNs)
selected_data_combined = selected_data_combined.dropna()

# Standardize the data (mean=0, variance=1)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(selected_data_combined)

# Perform PCA to reduce dimensionality
pca = PCA(n_components=1)
wealth_index = pca.fit_transform(scaled_data)

# Add the wealth index as a new column in your DataFrame
Pslm_data['Wealth_Index'] = wealth_index

# Optionally, you can sort or categorize the wealth index to create meaningful groups

# Save the updated DataFrame with the wealth index to a new CSV file
Pslm_data.to_csv('Pslm_data_with_wealth_index.csv', index=False)

#files.download('Pslm_data_with_wealth_index.csv')

variables_list1 = [
    "W_dkw_inspiped",
    "W_dkw_inshandpump",
    "W_dkw_insmotorpump",
    "W_dkw_insclosedwell",
    "W_dkw_insopenwell",
    "W_dkw_insprotsprng",
    "W_dkw_insunprsprng",
    "W_dkw_outpiped",
    "W_dkw_outhandpump",
    "W_dkw_outmotorpump",
    "W_dkw_outclosedwell",
    "W_dkw_outopenwell",
    "W_dkw_outprotsprng",
    "W_dkw_outunprsprng",
    "W_dkw_pond",
    "W_dkw_bottwater",
    "W_dkw_tanker",
    "W_dkw_filtration",
    "W_dkw_other",
    "W_dkw_piped",
    "W_dkw_handpump",
    "W_dkw_motorpump",
    "W_dkw_closedwell",
    "W_dkw_openwell",
    "W_dkw_protsprng",
    "W_dkw_unprsprng",
    "W_dkw_nodelivery",
    "W_time",
    "W_distance",
    "W_saferyes",
    "W_saferno",
    "W_saferdk",
    "W_safeboil",
    "W_safebleach",
    "W_safestrain",
    "W_safefilter",
    "W_safesettle",
    "W_safedk",
    "W_safeno",
    "W_toilet_notoilet",
    "W_toilet_flushpub",
    "W_toilet_flushtank",
    "W_toilet_flushpit",
    "W_toilet_flushopen",
    "W_toilet_raiselat",
    "W_toilet_pitlat",
    "W_toilet_other",
    "W_toiletshared",
    "W_toiletprivate",
    "D_ilcdled",
    "D_iclock",
    "D_iradio",
    "D_itelevsion",
    "D_irefrigerator",
    "D_ichair",
    "D_itable",
    "H_cooking_firewood",
    "H_cooking_gas",
    "H_cooking_lpg",
    "H_cooking_dung",
    "H_cooking_crop",
    "H_cooking_other",
    "H_floor_earth",
    "H_floor_ceramic",
    "H_floor_cement",
    "H_floor_bricks",
    "H_floor_other",
    "H_roof_rccrbc",
    "H_roof_wood",
    "H_roof_sheet",
    "H_roof_grader",
    "H_roof_other",
    "H_walls_burntbricks",
    "H_walls_mudbricks",
    "H_walls_wood",
    "H_walls_stones",
    "H_walls_other",
    "D_ibicycle",
    "D_imotorcyclescotter",
    "D_icar",
    "D_ivantruckbus"
]

selected_variables = Pslm_data[variables_list1]
means = selected_variables.mean()
std_devs = selected_variables.std()
# Count the number of missing values for each variable
missing_values = selected_variables.isna().sum()


selected_variables1 = Hies_data[variables_list1]
means1 = selected_variables1.mean()
std_devs1 = selected_variables1.std()
# Count the number of missing values for each variable
missing_values1 = selected_variables1.isna().sum()

Summary_data = pd.DataFrame({'Mean': means, 'Std':std_devs, 'Missing Values':missing_values,
                             'Mean1': means1, 'Std1':std_devs1, 'Missing Values1':missing_values1})

Summary_data