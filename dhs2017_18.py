# -*- coding: utf-8 -*-
"""DHS2017-18

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VbPgnF12dGRpriQqjhRJt5m6bYgysK5Y
"""

from google.colab import drive
import sys

# Mount Google Drive
drive.mount('/content/drive')

# Get the absolute path of the current folder
abspath_curr = '/content/drive/My Drive/Colab Notebooks/'

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import numpy as np
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder

dhs_data = pd.read_stata(abspath_curr + '/data/PKHR71DT/PKHR71FL.DTA')

dhs_data1 = pd.read_stata(abspath_curr + '/data/PKBR71DT/PKBR71FL.DTA', convert_categoricals=False)

# Create variables domestic, house, and land
dhs_data['domestic'] = 0
dhs_data['house'] = 0
dhs_data['land'] = 0

dhs_data = dhs_data[dhs_data['hv015'] != 1]

# Check conditions and update the new columns

df2_condition = (dhs_data1['v716'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v704'] == 95) & (dhs_data1['v003'] == 12)
dhs_data.loc[df2_condition, 'domestic'] = 1

df2_condition = (dhs_data1['v745a'] >= 1) & (dhs_data1['v745a'] <= 3)
dhs_data.loc[df2_condition, 'house'] = 1

df2_condition = (dhs_data1['v745b'] >= 1) & (dhs_data1['v745b'] <= 3)
dhs_data.loc[df2_condition, 'land'] = 1

df2_condition = (dhs_data1['v740'] == 1)
dhs_data.loc[df2_condition, 'land'] = 1

# Step 1: If (hv012=0), hv012=hv013
dhs_data.loc[dhs_data['hv012'] == 0, 'hv012'] = dhs_data['hv013']

# Step 2: If (hv216>0), memsleep=trunc(hv012/hv216)
dhs_data['memsleep'] = dhs_data.apply(lambda row: int(row['hv012'] / row['hv216']) if row['hv216'] > 0 else row['hv012'], axis=1)

# Step 3: If (hv216=0), memsleep=hv012
dhs_data.loc[dhs_data['hv216'] == 0, 'memsleep'] = dhs_data['hv012']

# Step 4: If (memsleep>=98), memsleep=98
dhs_data.loc[dhs_data['memsleep'] >= 98, 'memsleep'] = 98

# Display the resulting DataFrame or save it to a CSV file
print(dhs_data)

# If you want to save the modified DataFrame to a CSV file, you can use:
# df.to_csv('output_data.csv', index=False)

def addColumnsFromColumnsWithCategory(df, columns):
    addedColumns = []
    for column in columns:
        categories = set(str(x) for x in df[column] if str(x)!='nan' and str(x)!='other' )
        # print(column, " -> ", categories)
        for category in categories:
            addedColumns.append(category)
            df[category] = df[column].apply(lambda x: 1 if x == category else 0)
    return [df, addedColumns]


def addColumnsFromColumnsWithYesNo(df, columns):
    for column in columns:
        df[column] = df[column].apply(lambda x: 1 if x=='yes'  else 0)
    return df

categoryColumns = ['hv201', 'hv205', 'hv226', 'hv213', 'hv214', 'hv215']

addColsResults = addColumnsFromColumnsWithCategory(dhs_data , categoryColumns)
dhs_data_req = addColsResults[0]

#dhs_data_req = dhs_data_req.apply(lambda x: 1 if x=='yes'  else 0)
addedColumns = addColsResults[1]
addColsResults[1]
#dhs_data_req = addColumnsFromColumnsWithYesNo(dhs_data_req, yesNoColumns)

#print(addedColumns)
#reqColumns = addedColumns + yesNoColumns
print(dhs_data_req)

from pandas.core.apply import normalize_keyword_aggregation
means = list(dhs_data_req[addedColumns].mean())
sd = list(dhs_data_req[addedColumns].std())

newList = []

for i in range(len(addedColumns)):
  newList.append((addedColumns[i], means[i], sd[i]))
newDf = pd.DataFrame(newList, columns=('Variables', 'Mean', 'SD'))
newDf

dhs_data_rural = dhs_data_req[dhs_data_req['hv025']=='rural'][addedColumns]

means = list(dhs_data_rural.mean())
sd = list(dhs_data_rural.std())


newList = []

for i in range(len(addedColumns)):
    newList.append((addedColumns[i], means[i], sd[i]))
newDf = pd.DataFrame(newList, columns=('column', 'mean', 'sd'))
newDf

dhs_data_urban = dhs_data_req[dhs_data_req['hv025']=='urban'][addedColumns]

means = list(dhs_data_urban.mean())
sd = list(dhs_data_urban.std())


newList = []

for i in range(len(addedColumns)):
    newList.append((addedColumns[i], means[i], sd[i]))
newDf = pd.DataFrame(newList, columns=('column', 'mean', 'sd',))
newDf

# Create a new DataFrame or use your existing DataFrame (dhs_data)
# Assuming hv244 and hv245 are columns in your DataFrame
dhs_data['landarea'] = 0.0  # Initialize 'landarea' column with 0.0

# Convert 'hv245' to a numeric type (assuming it contains numeric values)
dhs_data['hv245'] = pd.to_numeric(dhs_data['hv245'], errors='coerce')

# Apply conditions to calculate 'landarea'
dhs_data.loc[dhs_data['hv244'] != 1, 'landarea'] = 0
dhs_data.loc[~dhs_data['hv245'].isna(), 'landarea'] = dhs_data['hv245'] / 10
dhs_data.loc[(dhs_data['hv244'] == 1) & (dhs_data['hv245'] == 0), 'landarea'] = 0.5
dhs_data.loc[(dhs_data['hv245'].isna()) | (dhs_data['hv245'] == 998), 'landarea'] = np.nan

yesNoColumns = ['hv206', 'hv207', 'hv208', 'hv209', 'hv210', 'hv225', 'hv243a', 'hv243b', 'hv211', 'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', 'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g', 'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g']

for column in yesNoColumns:
    dhs_data[column] = dhs_data[column].astype(str).str.strip().str.lower()
    dhs_data[column] = dhs_data[column].apply(lambda x: 1 if x == 'yes' else (1 if x == '1' else 0))

from pandas.core.tools.datetimes import unique
test= dhs_data['hv207']
test1= unique(test)
print(test1)

yesNoColumns = [ 'hv206', 'hv207', 'hv208', 'hv209', 'hv210','hv225', 'hv243a', 'hv243b', 'hv211', 'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', 'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g', 'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g','land', 'house', 'landarea','memsleep']
selected_variables = dhs_data[yesNoColumns]
# Fill missing values with 0
selected_variables = selected_variables.fillna(0)

# Calculate the mean for selected variables
means = selected_variables.mean()

# Calculate the standard deviation, excluding columns with no variability
std_devs = selected_variables.std()


# Count the number of missing values for each variable
missing_values = selected_variables.isna().sum()

# Create a summary DataFrame
Summary_data = pd.DataFrame({'Mean': means, 'Std': std_devs, 'Missing Values': missing_values})

# Display the summary data
print(Summary_data)

yesNoColumns = [ 'hv206', 'hv207', 'hv208', 'hv209', 'hv210','hv225', 'hv243a', 'hv243b', 'hv211', 'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', 'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g', 'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g','land', 'house', 'landarea','memsleep']
dhs_data_urban = dhs_data[dhs_data['hv025']=='urban']
selected_variables = dhs_data_urban[yesNoColumns]
# Fill missing values with 0
selected_variables = selected_variables.fillna(0)

# Calculate the mean for selected variables
means = selected_variables.mean()

# Calculate the standard deviation, excluding columns with no variability
std_devs = selected_variables.std()


# Count the number of missing values for each variable
missing_values = selected_variables.isna().sum()

# Create a summary DataFrame
Summary_data = pd.DataFrame({'Mean': means, 'Std': std_devs, 'Missing Values': missing_values})

# Display the summary data
print(Summary_data)

yesNoColumns = [ 'hv206', 'hv207', 'hv208', 'hv209', 'hv210','hv225', 'hv243a', 'hv243b', 'hv211', 'hv212', 'hv243c', 'hv243d', 'hv243e', 'hv247', 'sh121f', 'sh121g', 'sh121h', 'sh121i', 'sh121j', 'sh121k', 'sh121l', 'sh121m', 'sh121n', 'sh121o', 'sh121p', 'sh121r', 'sh122g', 'sh122i', 'sh122j', 'hv221', 'hv246a', 'hv246b', 'hv246c', 'hv246d', 'hv246e', 'hv246f', 'hv246g','land', 'house', 'landarea','memsleep']
dhs_data_rural = dhs_data[dhs_data['hv025']=='rural']
selected_variables = dhs_data_rural[yesNoColumns]
# Fill missing values with 0
selected_variables = selected_variables.fillna(0)

# Calculate the mean for selected variables
means = selected_variables.mean()

# Calculate the standard deviation, excluding columns with no variability
std_devs = selected_variables.std()


# Count the number of missing values for each variable
missing_values = selected_variables.isna().sum()

# Create a summary DataFrame
Summary_data = pd.DataFrame({'Mean': means, 'Std': std_devs, 'Missing Values': missing_values})

# Display the summary data
print(Summary_data)

column_name_mapping = {
    'piped into dwelling': 'hv201_11',
    'rainwater': 'hv201_51',
    'protected well': 'hv201_31',
    'piped to neighbor': 'hv201_13',
    'protected spring': 'hv201_41',
    'cart with small tank': 'hv201_71',
    'piped to yard/plot': 'hv201_12',
    'bottled water': 'hv201_91',
    'unprotected spring': 'hv201_42',
    'tube well or borehole': 'hv201_21',
    'filtration plant': 'hv201_96',
    'unprotected well': 'hv201_32',
    'river/dam/lake/ponds/stream/canal/irrigation channel': 'hv201_81',
    'tanker truck': 'hv201_61',
    'public tap/standpipe': 'hv201_14',
    'pit latrine with slab': 'hv205_22',
    'flush to piped sewer system': 'hv205_11',
    'ventilated improved pit latrine (vip)': 'hv205_21',
    'flush to septic tank': 'hv205_12',
    'hanging toilet/latrine': 'hv205_51',
    'flush to pit latrine': 'hv205_13',
    'pit latrine without slab/open pit': 'hv205_23',
    "flush, don't know where": 'hv205_15',
    'no facility/bush/field': 'hv205_61',
    'bucket toilet': 'hv205_41',
    'flush to somewhere else': 'hv205_14',
    'composting toilet': 'hv205_31',
    'straw/shrubs/grass': 'hv226_9',
    'kerosene': 'hv226_5',
    'electricity': 'hv226_1',
    'animal dung': 'hv226_11',
    'wood': 'hv226_8',
    'no food cooked in house': 'hv226_95',
    'natural gas': 'hv226_3',
    'agricultural crop': 'hv226_10',
    'coal, lignite': 'hv226_6',
    'charcoal': 'hv226_7',
    'lpg': 'hv226_2',
    'biogas': 'hv226_4',
    'cement': 'hv213_34',
    'chips/terrazzo': 'hv213_36',
    'mats': 'hv213_38',
    'earth/sand': 'hv213_11',
    'bricks': 'hv213_37',
    'parquet or polished wood': 'hv213_31',
    'wood planks': 'hv213_21',
    'vinyl or asphalt strips': 'hv213_32',
    'palm/bamboo': 'hv213_22',
    'dung': 'hv213_12',
    'carpet': 'hv213_35',
    'marble': 'hv213_39',
    'ceramic tiles': 'hv213_33',
    'stone with mud': 'hv214_23',
    'cement': 'hv214_31',
    'cane/palm/trunks': 'hv214_12',
    'plywood': 'hv214_25',
    'cement blocks': 'hv214_34',
    'bricks': 'hv214_33',
    'mud/stones': 'hv214_14',
    'stone with lime/cement': 'hv214_32',
    'wood planks/shingles': 'hv214_36',
    'covered adobe': 'hv214_35',
    'unbaked bricks/mud': 'hv214_21',
    'uncovered adobe': 'hv214_24',
    'no walls': 'hv214_11',
    'bamboo with mud': 'hv214_22',
    'dirt': 'hv214_13',
    'bamboo/sticks/mud': 'hv214_15',
    'reused wood': 'hv214_26',
    'calamine/cement fiber': 'hv215_35',
    'asbestos': 'hv215_31',
    'wood planks': 'hv215_23',
    'cement/rcc': 'hv215_37',
    'wood': 'hv215_34',
    'rustic mat': 'hv215_21',
    'sod/grass': 'hv215_13',
    'palm/bamboo': 'hv215_22',
    'metal': 'hv215_33',
    'thatch/palm leaf': 'hv215_12',
    'reinforced brick cement/rcc': 'hv215_32',
    'no roof': 'hv215_11',
    'roofing shingles': 'hv215_38',
    'cardboard': 'hv215_24',
    'ceramic tiles': 'hv215_36'
}

dhs_data_req = dhs_data_req.rename(columns=column_name_mapping)

# Concatenate the two DataFrames
combined_data = pd.concat([dhs_data[yesNoColumns], dhs_data_req], axis=1)

